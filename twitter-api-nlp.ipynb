{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loaded by default (c.f.: ~/.ipython/profile_default/startup)\n",
    "#Snippet location: ./anaconda3/share/jupyter/nbextensions/snippets/snippets.json\n",
    "from __future__ import division, print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_summary import DataFrameSummary\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Using the Twitter API:  Lab\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "<img src=\"https://snag.gy/RNAEgP.jpg\" width=\"600\">\n",
    "\n",
    "### Can we correctly identify which of these two old men tweeted what?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "---\n",
    "\n",
    "We are going to attempt to classify whether a tweet comes from Trump or Sanders.  This project involves multiple steps:\n",
    "- Create a developer account on Twitter\n",
    "- Create a method to pull a list of tweets from the Twitter API\n",
    "- Perform proper preprocessing on our text\n",
    "- Engineer sentiment feature in our dataset using TextBlob\n",
    "- Explore supervised classification techniques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter API Developer Registration\n",
    "---\n",
    "\n",
    "[Twitter Rest API](https://dev.twitter.com/rest/public)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an \"App\"\n",
    "\n",
    "---\n",
    "\n",
    "![](https://snag.gy/HPBQbJ.jpg)\n",
    "\n",
    "Go to Twitter and register an \"app\" [apps.twitter.com](https://apps.twitter.com/).\n",
    "\n",
    "> **Note**: For the required website field you can put a placeholder.\n",
    "\n",
    "After we set up our app, we will only need to reference the cooresponding keys Twitter generates for our app.  These are the keys that we will use with our application to communicate with the Twitter API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Python Twitter API library\n",
    "\n",
    "---\n",
    "\n",
    "We will use Python Twitter Tools. It makes pulling tweets simple: we only need to plug in our keys and start collecting data.<br>\n",
    "[Python Twitter Tools](http://mike.verdone.ca/twitter/).\n",
    "\n",
    "To install it, just run the next frame (there is no conda package)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install twitter python-twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Boring Twitter Rules\n",
    "---\n",
    "\n",
    "**Twitter notifies you they will rate limit your requests:**\n",
    "\n",
    ">When using application-only authentication, rate limits are determined globally for the entire application. If a method allows for 15 requests per rate limit window, then it allows you to make 15 requests per window — on behalf of your application. This limit is considered completely separately from per-user limits. https://dev.twitter.com/rest/public/rate-limiting\n",
    "\n",
    "Here's a quick overview of what Twitter says are \"the rules\":\n",
    "\n",
    "![](https://snag.gy/yJ6vIH.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About those Keys: OAuth Review\n",
    "---\n",
    "\n",
    "![](https://g.twimg.com/dev/documentation/image/appauth_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Application Keys\n",
    "---\n",
    "\n",
    "Note application keys, we will use it to connect to Twitter account and mine tweets from the official Bernie Sanders and Donald Trump twitter accounts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `TweetMiner` class structure\n",
    "\n",
    "---\n",
    "\n",
    "The following will provide connectivity to twitter. The class has the ability to make requests and can eventually transform the JSON responses into DataFrames.\n",
    "\n",
    "\n",
    "> **Note:** \"request_limit\" is used in this class to limit the number of tweets that are pulled per instance request.  Setting it to something lower until we've worked the bugs out of the request, and captured the data we want, is essential to avoiding the rate limit blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter\n",
    "import re\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "twitter_keys = {\n",
    "    'consumer_key':        'lA6UQplM5sxIAIr83ueUl9sgE',\n",
    "    'consumer_secret':     'f94t4BD6Vj7aCVkX1qAIVwsP4x69J2vXvm61lTIuwb9GfmdsuP',\n",
    "    'access_token_key':    '1203210464-dYE1FvoUx1GjVcoyok3U1brWDpELBJEcSNGC1OC',\n",
    "    'access_token_secret': 'kuVli1j010RPdYTboz9iIyp8QxX6PqRApdi49baLqBgzo'\n",
    "}\n",
    "\n",
    "api = twitter.Api(\n",
    "    consumer_key=twitter_keys['consumer_key'],\n",
    "    consumer_secret=twitter_keys['consumer_secret'],\n",
    "    access_token_key=twitter_keys['access_token_key'],\n",
    "    access_token_secret=twitter_keys['access_token_secret']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetMiner(object):\n",
    "\n",
    "    result_limit = 20\n",
    "    api = False\n",
    "    data = []\n",
    "\n",
    "    twitter_keys = {\n",
    "        'consumer_key':        'KmN03M1X1pImZ43sqdIu4yfnE',\n",
    "        'consumer_secret':     'ePlIrIX5VXbZnO7DBu1RbFlw5lOai9dQr9n5TZb6vxnIdrr5Fz',\n",
    "        'access_token_key':    '185036086-Q7K5IjuSoQZJwSIqD0wyHf6t62iPKatmfaPkriAM',\n",
    "        'access_token_secret': 'cYpQz3xWHQbLplOj8iSeiNSOmMcsTOXmWcKMrJ9buLj5d'\n",
    "    }\n",
    "\n",
    "    def __init__(self, keys_dict, api, result_limit=20):\n",
    "\n",
    "        self.api = api\n",
    "        self.twitter_keys = keys_dict\n",
    "\n",
    "        self.result_limit = result_limit\n",
    "\n",
    "    def mine_user_tweets(self, user=\"dyerrington\", mine_rewteets=False, max_pages=5):\n",
    "\n",
    "        data = []\n",
    "        last_tweet_id = False\n",
    "        page = 1\n",
    "\n",
    "        while page <= max_pages:\n",
    "\n",
    "            if last_tweet_id:\n",
    "                statuses = self.api.GetUserTimeline(\n",
    "                    screen_name=user, count=self.result_limit, max_id=last_tweet_id - 1)\n",
    "            else:\n",
    "                statuses = self.api.GetUserTimeline(\n",
    "                    screen_name=user, count=self.result_limit)\n",
    "\n",
    "            for item in statuses:\n",
    "\n",
    "                mined = {\n",
    "                    'tweet_id':        item.id,\n",
    "                    'handle':          item.user.name,\n",
    "                    'retweet_count':   item.retweet_count,\n",
    "                    'text':            item.text,\n",
    "                    'mined_at':        datetime.datetime.now(),\n",
    "                    'created_at':      item.created_at,\n",
    "                }\n",
    "\n",
    "                last_tweet_id = item.id\n",
    "                data.append(mined)\n",
    "\n",
    "            page += 1\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the class\n",
    "---\n",
    "\n",
    "Pass the keys dictionary and the api as arguments.\n",
    "\n",
    "**Check:** call the object's `mine_user_tweets()` method, providing a user to pull the tweets of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "miner = TweetMiner(twitter_keys, api, result_limit=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanders = miner.mine_user_tweets(user=\"berniesanders\", max_pages=5)\n",
    "donald = miner.mine_user_tweets(user=\"realDonaldTrump\", max_pages=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tweet_id': 995034893687119872, 'handle': 'Bernie Sanders', 'retweet_count': 101, 'text': 'RT @RichLazerPHL: Proud to have the endorsement of @BernieSanders in #PA05 -- I look forward to fighting alongside him for working families…', 'mined_at': datetime.datetime(2018, 5, 14, 16, 53, 26, 673396), 'created_at': 'Fri May 11 20:16:20 +0000 2018'}\n"
     ]
    }
   ],
   "source": [
    "print(sanders[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tweet_id': 996046634864791557, 'handle': 'Donald J. Trump', 'retweet_count': 3709, 'text': '#USEmbassyJerusalem https://t.co/f1SFvrkcAH', 'mined_at': datetime.datetime(2018, 5, 14, 16, 53, 28, 47705), 'created_at': 'Mon May 14 15:16:38 +0000 2018'}\n"
     ]
    }
   ],
   "source": [
    "print(donald[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the tweet ouputs to a pandas DataFrame\n",
    "\n",
    "> *This is as easy as passing it to the DataFrame constructor!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>handle</th>\n",
       "      <th>mined_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fri May 11 20:16:20 +0000 2018</td>\n",
       "      <td>Bernie Sanders</td>\n",
       "      <td>2018-05-14 16:53:26.673396</td>\n",
       "      <td>101</td>\n",
       "      <td>RT @RichLazerPHL: Proud to have the endorsemen...</td>\n",
       "      <td>995034893687119872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sun May 06 19:29:26 +0000 2018</td>\n",
       "      <td>Bernie Sanders</td>\n",
       "      <td>2018-05-14 16:53:26.673404</td>\n",
       "      <td>170</td>\n",
       "      <td>Bernie just returned from a campaign swing wit...</td>\n",
       "      <td>993211151373819909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sun May 06 19:29:06 +0000 2018</td>\n",
       "      <td>Bernie Sanders</td>\n",
       "      <td>2018-05-14 16:53:27.046898</td>\n",
       "      <td>261</td>\n",
       "      <td>Thanks to all the people who have provided sup...</td>\n",
       "      <td>993211068179730433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sun May 06 19:28:11 +0000 2018</td>\n",
       "      <td>Bernie Sanders</td>\n",
       "      <td>2018-05-14 16:53:27.046905</td>\n",
       "      <td>798</td>\n",
       "      <td>If we are going to turn this country around po...</td>\n",
       "      <td>993210837711060993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sat May 05 19:24:16 +0000 2018</td>\n",
       "      <td>Bernie Sanders</td>\n",
       "      <td>2018-05-14 16:53:27.285391</td>\n",
       "      <td>254</td>\n",
       "      <td>RT @AriRabinHavt: . @edwardsforpa agrees with ...</td>\n",
       "      <td>992847463034900481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at          handle                   mined_at  \\\n",
       "0  Fri May 11 20:16:20 +0000 2018  Bernie Sanders 2018-05-14 16:53:26.673396   \n",
       "1  Sun May 06 19:29:26 +0000 2018  Bernie Sanders 2018-05-14 16:53:26.673404   \n",
       "2  Sun May 06 19:29:06 +0000 2018  Bernie Sanders 2018-05-14 16:53:27.046898   \n",
       "3  Sun May 06 19:28:11 +0000 2018  Bernie Sanders 2018-05-14 16:53:27.046905   \n",
       "4  Sat May 05 19:24:16 +0000 2018  Bernie Sanders 2018-05-14 16:53:27.285391   \n",
       "\n",
       "   retweet_count                                               text  \\\n",
       "0            101  RT @RichLazerPHL: Proud to have the endorsemen...   \n",
       "1            170  Bernie just returned from a campaign swing wit...   \n",
       "2            261  Thanks to all the people who have provided sup...   \n",
       "3            798  If we are going to turn this country around po...   \n",
       "4            254  RT @AriRabinHavt: . @edwardsforpa agrees with ...   \n",
       "\n",
       "             tweet_id  \n",
       "0  995034893687119872  \n",
       "1  993211151373819909  \n",
       "2  993211068179730433  \n",
       "3  993210837711060993  \n",
       "4  992847463034900481  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(sanders).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Create the training data\n",
    "\n",
    "---\n",
    "\n",
    "Let's get our \"mined\" data from the Twitter API.  \n",
    "\n",
    "1. Mine Trump tweets\n",
    "- Create a tweet DataFrame\n",
    "- Mine Sanders tweets\n",
    "- Append the results to our DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only need to \"instantiate\" once.  Then we can call mine_user_tweets as much as we want.\n",
    "miner = TweetMiner(twitter_keys, api, result_limit=400)\n",
    "trump_tweets = miner.mine_user_tweets(\"realDonaldTrump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 6)\n"
     ]
    }
   ],
   "source": [
    "trump_df = pd.DataFrame(trump_tweets)\n",
    "print(trump_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernie_tweets = miner.mine_user_tweets('berniesanders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(998, 6)\n"
     ]
    }
   ],
   "source": [
    "bernie_df = pd.DataFrame(bernie_tweets)\n",
    "print(bernie_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1998, 6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.concat([trump_df, bernie_df], axis=0)\n",
    "tweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Any interesting ngrams going on with Trump?\n",
    "---\n",
    "\n",
    "Set up a vectorizer from sklearn and fit the text of Trump's tweets with an ngram range from 2 to 4. <br>\n",
    "Lets figure out what the most common ngrams are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('https co', 817),\n",
       " ('of the', 96),\n",
       " ('in the', 68),\n",
       " ('to the', 57),\n",
       " ('will be', 44),\n",
       " ('fake news', 43),\n",
       " ('for the', 40),\n",
       " ('on the', 39),\n",
       " ('our country', 35),\n",
       " ('at the', 35),\n",
       " ('and the', 30),\n",
       " ('we are', 30),\n",
       " ('thank you', 30),\n",
       " ('want to', 29),\n",
       " ('all of', 29),\n",
       " ('honor to', 29),\n",
       " ('with the', 28),\n",
       " ('united states', 24),\n",
       " ('was my', 24),\n",
       " ('the https', 24)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "# We can use the TfidfVectorizer to find ngrams for us\n",
    "vect = TfidfVectorizer(ngram_range=(2, 4))\n",
    "\n",
    "# Pulls all of trumps tweet text's into one giant string\n",
    "summaries = \"\".join(trump_df['text'])\n",
    "ngrams_summaries = vect.build_analyzer()(summaries)\n",
    "\n",
    "Counter(ngrams_summaries).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the ngrams for Bernie Sanders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('https co', 445),\n",
       " ('health care', 139),\n",
       " ('of the', 65),\n",
       " ('bernie sanders', 57),\n",
       " ('in the', 52),\n",
       " ('to the', 51),\n",
       " ('we must', 44),\n",
       " ('we need', 41),\n",
       " ('for the', 41),\n",
       " ('we are', 37),\n",
       " ('this country', 36),\n",
       " ('for all', 36),\n",
       " ('is not', 36),\n",
       " ('millions of', 34),\n",
       " ('on the', 30),\n",
       " ('going to', 29),\n",
       " ('it is', 29),\n",
       " ('and the', 28),\n",
       " ('is the', 27),\n",
       " ('should be', 27)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can use the TfidfVectorizer to find ngrams for us\n",
    "vect = TfidfVectorizer(ngram_range=(2, 4))\n",
    "\n",
    "# Pulls all of trumps tweet text's into one giant string\n",
    "summaries = \"\".join(bernie_df['text'])\n",
    "ngrams_summaries = vect.build_analyzer()(summaries)\n",
    "\n",
    "Counter(ngrams_summaries).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the tweets and building a model\n",
    "\n",
    "---\n",
    "\n",
    "To do classfication we will need to convert the tweets into a set of features.\n",
    "\n",
    "***Will need to:***\n",
    "- Vectorize input text data.\n",
    "- Intialize a model (try Logistic regression).\n",
    "- Train / Predict / cross-validate.\n",
    "- Evaluate the performance of the model.\n",
    "\n",
    "> I noticed that there are website links in the tweets so we can do some additional preprocessing before building the model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the textacy package to do some more comprehensive preprocessing\n",
    "# http://textacy.readthedocs.io/en/latest/\n",
    "from textacy.preprocess import preprocess_text\n",
    "\n",
    "tweet_text = tweets['text'].values\n",
    "clean_text = [preprocess_text(x, fix_unicode=True, lowercase=True, transliterate=False,\n",
    "                              no_urls=True, no_emails=True, no_phone_numbers=True, no_currency_symbols=True,\n",
    "                              no_punct=True, no_accents=True)\n",
    "              for x in tweet_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#USEmbassyJerusalem https://t.co/f1SFvrkcAH'\n",
      " 'Big day for Israel. Congratulations!'\n",
      " 'U.S. Embassy opening in Jerusalem will be covered live on @FoxNews &amp; @FoxBusiness. Lead up to 9:00 A.M. (eastern) e… https://t.co/VncqHcDmLG']\n"
     ]
    }
   ],
   "source": [
    "print(tweet_text[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['usembassyjerusalem url', 'big day for israel congratulations', 'u s embassy opening in jerusalem will be covered live on foxnews foxbusiness lead up to 9 00 a m eastern e url']\n"
     ]
    }
   ],
   "source": [
    "print(clean_text[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5005005005005005\n"
     ]
    }
   ],
   "source": [
    "# target is the handle.\n",
    "# make trump 1 and sanders 0\n",
    "y = tweets['handle'].map(lambda x: 1 if x == 'Donald J. Trump' else 0).values\n",
    "print(np.mean(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1998, 2000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Preprocess our text data to Tfidf\n",
    "tfv = TfidfVectorizer(ngram_range=(1, 4), max_features=2000)\n",
    "X = tfv.fit_transform(clean_text).todense()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.81       0.89       0.915      0.925      0.885      0.93\n",
      " 0.93       0.93       0.90452261 0.89447236]\n",
      "0.9013994974874372\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cross-validate the accuracy:\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(LogisticRegression(), X, y, cv=10)\n",
    "\n",
    "import numpy as np\n",
    "print(accuracies)\n",
    "print(np.mean(accuracies))\n",
    "\n",
    "# Setup logistic regression (or try another classification method here)\n",
    "estimator = LogisticRegression()\n",
    "estimator.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very good accuracy considering the baseline is 50%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the predicted probability for a random Sanders and Trump tweet\n",
    "---\n",
    "\n",
    "Below are a couple of tweets from both Sanders and Trump.\n",
    "\n",
    "Estimate the predicted probability of being trump for the two tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.71410576, 0.28589424],\n",
       "       [0.285973  , 0.714027  ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prep our source as TfIdf vectors\n",
    "source_test = [\n",
    "    \"Demanding that the wealthy and the powerful start paying their fair share of taxes that's exactly what the American people want.\",\n",
    "    \"Crooked Hillary is spending tremendous amounts of Wall Street money on false ads against me. She is a very dishonest person!\"\n",
    "]\n",
    "\n",
    "############\n",
    "# NOTE:  Do not re-initialize the tfidf vectorizor or the feature space willbe overwritten and\n",
    "# the transform will not match the number of features trained the model on.\n",
    "#\n",
    "# This is why we only need to \"transform\" since we have already \"fit\" previously\n",
    "#\n",
    "####\n",
    "\n",
    "Xtest = tfv.transform(source_test)\n",
    "\n",
    "# Predict using previously trained logist regression `estimator`\n",
    "estimator.predict_proba(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 1st column is probability of being Bernie, and 2nd Trump. The classifier is getting it right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull tweets for some new users.\n",
    "\n",
    "Experiment using more data. The API will not like it if we blow through their limits so we have to  be careful and try to grab only what we need one time, then work on the copy of the objects that are returned.\n",
    "\n",
    "> NOTE: Read the documentation about rate limits to see the options available in the API to avoid this problem.\n",
    "\n",
    "**Pull tweets for more than two different users **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mining tweets for:  dril\n",
      "Mining tweets for:  LaziestCanine\n",
      "Mining tweets for:  ch000ch\n"
     ]
    }
   ],
   "source": [
    "# We deviate from trump / sanders using student tweets here to illustrate the NLP pipeine with twitter data\n",
    "\n",
    "twitter_handles = [\"dril\", \"LaziestCanine\", 'ch000ch']\n",
    "tweets = {}\n",
    "\n",
    "for twitter_handle in twitter_handles:\n",
    "    print(\"Mining tweets for: \", twitter_handle)\n",
    "    miner = TweetMiner(twitter_keys, api, result_limit=500)\n",
    "    tweets[twitter_handle] = miner.mine_user_tweets(\n",
    "        user=twitter_handle, max_pages=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5985, 6)\n"
     ]
    }
   ],
   "source": [
    "multi = pd.DataFrame(tweets['dril'])\n",
    "multi = multi.append(pd.DataFrame(tweets['LaziestCanine']))\n",
    "multi = multi.append(pd.DataFrame(tweets['ch000ch']))\n",
    "\n",
    "print(multi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chuuch      1996\n",
       "wint        1995\n",
       "Lazy dog    1994\n",
       "Name: handle, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi.handle.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a multi-class classification model to distinguish between the users.\n",
    "\n",
    "Try a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text = multi['text'].values\n",
    "clean_text = [preprocess_text(x, fix_unicode=True, lowercase=True, transliterate=False,\n",
    "                              no_urls=True, no_emails=True, no_phone_numbers=True, no_currency_symbols=True,\n",
    "                              no_punct=True, no_accents=True)\n",
    "              for x in tweet_text]\n",
    "\n",
    "y = multi['handle'].map(\n",
    "    lambda x: 0 if x == 'wint' else 1 if x == 'Lazy dog' else 2).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(ngram_range=(1, 3), max_features=2500)\n",
    "X = tfv.fit_transform(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 250 out of 250 | elapsed:    5.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=7, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=250, verbose=1)\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 250 out of 250 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF: 0.7032293986636972\n",
      "KNN: 0.4142538975501114\n"
     ]
    }
   ],
   "source": [
    "# Random forest score:\n",
    "print('RF:', rf.score(X_test, y_test))\n",
    "print('KNN:', knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chuuch      0.333500\n",
       "wint        0.333333\n",
       "Lazy dog    0.333166\n",
       "Name: handle, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline score:\n",
    "multi.handle.value_counts()/multi.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_yhat = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a confusion matrix and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.10      0.18       630\n",
      "          1       0.36      0.80      0.50       585\n",
      "          2       0.51      0.36      0.42       581\n",
      "\n",
      "avg / total       0.52      0.41      0.36      1796\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(y_test, rf_yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 64 458 108]\n",
      " [ 23 470  92]\n",
      " [ 10 361 210]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "print(confusion_matrix(y_test, rf_yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most and least \"distinctive\" tweets for each user?\n",
    "\n",
    "To find this, we check the tweet that has the highest (correct) predicted probability of being that user's tweet for each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 250 out of 250 | elapsed:    8.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 250 out of 250 | elapsed:    1.1s finished\n"
     ]
    }
   ],
   "source": [
    "rf.fit(X, y)\n",
    "pp = rf.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.952     , 0.012     , 0.036     ],\n",
       "       [0.47577615, 0.46609291, 0.05813095],\n",
       "       [0.888     , 0.02      , 0.092     ],\n",
       "       [0.788     , 0.04      , 0.172     ],\n",
       "       [0.84364824, 0.08031591, 0.07603586]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pd.DataFrame(pp, columns=['dril_pp', 'laziestcanine_pp', 'ch000ch_pp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5985, 6) (5985, 3)\n"
     ]
    }
   ],
   "source": [
    "print(multi.shape, pp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>created_at</th>\n",
       "      <th>handle</th>\n",
       "      <th>mined_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>index</th>\n",
       "      <th>dril_pp</th>\n",
       "      <th>laziestcanine_pp</th>\n",
       "      <th>ch000ch_pp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Wed May 09 20:33:41 +0000 2018</td>\n",
       "      <td>wint</td>\n",
       "      <td>2018-05-10 20:50:30.039267</td>\n",
       "      <td>2731</td>\n",
       "      <td>gathering data on various of bastards</td>\n",
       "      <td>994314487682555904</td>\n",
       "      <td>0</td>\n",
       "      <td>0.952000</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>0.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Tue May 08 19:39:35 +0000 2018</td>\n",
       "      <td>wint</td>\n",
       "      <td>2018-05-10 20:50:30.039281</td>\n",
       "      <td>631</td>\n",
       "      <td>https://t.co/rbM8Cj6jkM</td>\n",
       "      <td>993938484556640256</td>\n",
       "      <td>1</td>\n",
       "      <td>0.475776</td>\n",
       "      <td>0.466093</td>\n",
       "      <td>0.058131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                      created_at handle                   mined_at  \\\n",
       "0      0  Wed May 09 20:33:41 +0000 2018   wint 2018-05-10 20:50:30.039267   \n",
       "1      1  Tue May 08 19:39:35 +0000 2018   wint 2018-05-10 20:50:30.039281   \n",
       "\n",
       "   retweet_count                                   text            tweet_id  \\\n",
       "0           2731  gathering data on various of bastards  994314487682555904   \n",
       "1            631                https://t.co/rbM8Cj6jkM  993938484556640256   \n",
       "\n",
       "   index   dril_pp  laziestcanine_pp  ch000ch_pp  \n",
       "0      0  0.952000          0.012000    0.036000  \n",
       "1      1  0.475776          0.466093    0.058131  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_pp = pd.concat([multi.reset_index(), pp.reset_index()], axis=1)\n",
    "tweets_pp.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most dril: @formida_poupon shut hte fuck up\n",
      "Least dril: @StaggMack  Puerto Rican Pisser\n"
     ]
    }
   ],
   "source": [
    "print('Most dril:', tweets_pp[tweets_pp.handle == 'wint'].sort_values(\n",
    "    'dril_pp', ascending=False).text.values[0])\n",
    "print('Least dril:', tweets_pp[tweets_pp.handle == 'wint'].sort_values(\n",
    "    'dril_pp', ascending=True).text.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most LaziestCanine: you vs. the guy she tells you not to worry about https://t.co/yhs18lGqvJ\n",
      "Least LaziestCanine: RT @CouRageJD: https://t.co/CVvPR4PF1n\n"
     ]
    }
   ],
   "source": [
    "print('Most LaziestCanine:', tweets_pp[tweets_pp.handle == 'Lazy dog'].sort_values(\n",
    "    'laziestcanine_pp', ascending=False).text.values[0])\n",
    "print('Least LaziestCanine:', tweets_pp[tweets_pp.handle == 'Lazy dog'].sort_values(\n",
    "    'laziestcanine_pp', ascending=True).text.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most chuuch: @BuckyIsotope damnit\n",
      "Least chuuch: https://t.co/HJsaeyaRCd\n"
     ]
    }
   ],
   "source": [
    "print('Most chuuch:', tweets_pp[tweets_pp.handle == 'chuuch'].sort_values(\n",
    "    'ch000ch_pp', ascending=False).text.values[0])\n",
    "print('Least chuuch:', tweets_pp[tweets_pp.handle == 'chuuch'].sort_values(\n",
    "    'ch000ch_pp', ascending=True).text.values[0])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
